<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>A Pelican Blog - Till Bergmann</title><link>/</link><description></description><lastBuildDate>Sun, 15 May 2016 00:00:00 +0000</lastBuildDate><item><title>Inferring the posteriors in LDA through Gibbs sampling</title><link>/lda-gibbs-toy.html</link><description>&lt;p&gt;In my last blog post, which was about a million years ago, I described the generative nature of LDA and left the interferential step open. In this blog post, I will explain one method to calculate estimations of the topic distribution θ and the term distribution ϕ. This approach, first formulated by Griffiths and Steyvers (2004) in the context of LDA, is to use Gibbs sampling, a common algorithm within the Markov Chain Monte Carlo (MCMC) family of sampling algorithms. Before applying Gibbs sampling directly to LDA, I will first give a short introduction to Gibbs sampling more generally.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Till Bergmann</dc:creator><pubDate>Sun, 15 May 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:None,2016-05-15:/lda-gibbs-toy.html</guid><category>Python</category><category>Gibbs Sampling</category><category>LDA</category></item></channel></rss>