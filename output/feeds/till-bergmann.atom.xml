<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>A Pelican Blog - Till Bergmann</title><link href="/" rel="alternate"></link><link href="/feeds/till-bergmann.atom.xml" rel="self"></link><id>/</id><updated>2016-05-15T00:00:00+00:00</updated><entry><title>Inferring the posteriors in LDA through Gibbs sampling</title><link href="/lda-gibbs-toy.html" rel="alternate"></link><published>2016-05-15T00:00:00+00:00</published><updated>2016-05-15T00:00:00+00:00</updated><author><name>Till Bergmann</name></author><id>tag:None,2016-05-15:/lda-gibbs-toy.html</id><summary type="html">&lt;p&gt;In my last blog post, which was about a million years ago, I described the generative nature of LDA and left the interferential step open. In this blog post, I will explain one method to calculate estimations of the topic distribution θ and the term distribution ϕ. This approach, first formulated by Griffiths and Steyvers (2004) in the context of LDA, is to use Gibbs sampling, a common algorithm within the Markov Chain Monte Carlo (MCMC) family of sampling algorithms. Before applying Gibbs sampling directly to LDA, I will first give a short introduction to Gibbs sampling more generally.&lt;/p&gt;</summary><content type="html">&lt;p&gt;{% notebook notebooks/lda-gibbs.ipynb %}&lt;/p&gt;</content><category term="Python"></category><category term="Gibbs Sampling"></category><category term="LDA"></category></entry></feed>